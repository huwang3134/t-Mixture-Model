---
title: "t Mixture Model"
output:
  github_document:
    pandoc_args: --webtex
---

##Model

Say we have a set of  $p$ dimensional observations $\{\boldsymbol{X_1}, \dots, \boldsymbol{X_n}\}$. A t Mixture Model (tMM) is a parametric probabilistic model that assumes each observation,  $\boldsymbol{X_i}$, is generated from a finite mixture of multivariate t distributions. Unlike Gaussian Mixture models (GMMs), tMMs are robust to outliers, and as such can be used as a robust method of clustering. This robustness, however, comes at a computational expense.

A random vector $\boldsymbol{T}$ of dimension $p$ is said to follow a multivariate t distribution, $\boldsymbol {T} \sim t(\boldsymbol {\mu_g}, \boldsymbol{ \Sigma_g}, \nu_g)$, if the probability density function of $T$ is:


$$f_{\boldsymbol{T}}(\boldsymbol{x}| \boldsymbol{\mu}, \boldsymbol{\Sigma}, \nu) = \frac{\Gamma[(\nu + p)/2]}{\Gamma(\nu/2)\nu^{p/2}\pi^{p/2} |\boldsymbol{\Sigma}|^{\frac{1}{2}}}  \left[1 + \frac{1}{\nu}(\boldsymbol{x} - \boldsymbol{\mu})^T \boldsymbol {\Sigma^{-1}}(\boldsymbol{x} - \boldsymbol{\mu}) \right]^{-(\nu+p)/2}$$

Where $\boldsymbol{\mu}$ is a $p \times 1$ location vector, $\boldsymbol{\Sigma}$ is a $p \times p$ positive definite scale matrix and $\nu$, the degrees of freedom, is a natural number.

A random vector $\boldsymbol{\tau}$ is said to be $G$ component finite mixture of multivariate $t$ distributions if the probability density function of $\boldsymbol{\tau}$ is given by:

$$
f_{\boldsymbol{\tau}}\left(\boldsymbol {x} | \pi_g, \boldsymbol{\mu_g}, \boldsymbol{\Sigma_g}, \nu_g; g = 1,\dots G \right) = \sum_{g=1}^G \pi_g f_{\boldsymbol{T_g}}(\boldsymbol{x} | \boldsymbol{\mu_g}, \boldsymbol{\Sigma_g}, \nu_g ) 
$$

where

$$
\boldsymbol {T_g} \overset{\text{ind}}{\sim} t(\boldsymbol {\mu_g}, \boldsymbol {\Sigma_g}, \nu_g), \quad \sum_{g=1}^G \pi_g = 1, \quad 0 \leq \pi_g \leq 1, \forall g
$$

##Parameter Estimation

The parameters of the model are $\pi_g, \boldsymbol{\mu_g}, \boldsymbol{\Sigma_g}, \nu_g$ for $g = 1, \dots, G$ ($G$, the number of components, is assumed to be known) and are estimated using maximum likelihood. However, the likelihood function is difficult to maximize using standard methods and so a special case of the minorize-maximize algorithm, the Expectation Maximization (EM) algorithm, is used. The EM algorithm is an iterative algorithm in which the parameters of the model are updated at each iteration (divided into an E-step and then an M-step) until a specified stopping criterion. Like many iterative algorithms the EM algorithm requires a starting estimate for the parameters. One of the advantages of the EM algorithm is that the parameter estimates at each iteration of the algorithm are guaranteed to yield a greater log likelihood than the parameter estimates of the previous iteration, a consequence of Jensen's Inequality. The parameter estimates at each iteration are as follows:

**E step**

The values computed in the E-step are to compute the parameter estimates in the M-step.

$$
E[Z_{ig}|\boldsymbol {X_i}] =  \frac{\pi_g f_{\boldsymbol {T_g}}(\boldsymbol {X_i})}{\sum_{g=1}^G \pi_g f_{\boldsymbol {T_g}}(\boldsymbol {X_i})}
$$ 

$$
k_i = \text{Argmax}_{\boldsymbol g} (E[Z_{ig}|\boldsymbol {X_i}]) 
$$


$$
E \left[U_i| \boldsymbol{X_i}, \boldsymbol{Z_i}  \right] = \frac{v_{k_i} + p}{ \nu_{k_i} + (\boldsymbol{X_i} - \boldsymbol{\mu_{k_i}})^T \boldsymbol{\Sigma_g}^{-1}(\boldsymbol{X_i} - \boldsymbol{\mu_{k_i}})}
$$

$$
E \left [\log(U_i)| \boldsymbol { X_i}, \boldsymbol {Z_i } \right]  = \psi \left[\frac{1}{2}(\nu_{k_i} + p) \right] + \log \left[E [U_i| \boldsymbol  {X_i}, \boldsymbol {Z_i}] \right] - \log\left(\frac{1}{2}(v_{k_i} + p)\right)
$$

$\psi(\cdot )$ denotes the digamma function.

**M step**

$$
\hat \pi_g = \frac{\sum_{i=1}^n E[Z_{ig}|\boldsymbol {X_i}] }{n} = \text{Ave}_{i=1, \dots, n}(E[Z_{ig}|\boldsymbol{X_i}])
$$

$$
\hat {\boldsymbol {\mu_g}} = \frac{\sum_{i=1}^n E[Z_{ig}|\boldsymbol {X_i}]  E \left [U_i| \boldsymbol  {X_i}, \boldsymbol {Z_i}\right] \boldsymbol {X_i} } {\sum_{i=1}^n E[Z_{ig}|\boldsymbol {X_i}]  E \left [U_i| \boldsymbol  {X_i}, \boldsymbol {Z_i} \right] }
$$

$$
\hat {\boldsymbol {\Sigma_g}} = \frac{\sum_{i=1}^n E[Z_{ig}|\boldsymbol {X_i}]E \left [U_i| \boldsymbol  {X_i}, \boldsymbol {Z_i} \right](\boldsymbol {X_i} - {\boldsymbol {\hat \mu_g}})(\boldsymbol {X_i} - {\boldsymbol {\hat \mu_g}})^T  }{\sum_{i=1}^n  E[Z_{ig}|\boldsymbol {X_i}] }
$$

The estimate for the $\nu_g$'s must be found by maximizing the following univariate function for $g = 1, \dots, G$.

$$
Q_2(\nu_g) = \sum_{i=1}^n  E[Z_{ig}|\boldsymbol {X_i}] \left( \frac{\nu_g}{2}\log\left(\frac{\nu_g}{2} \right) - \log \left ( \Gamma (\frac{\nu_g}{2}) \right) + \left(\frac{\nu_g}{2} - 1 \right) E \left [ \log(U_i) |  \boldsymbol {X_i}, \boldsymbol {Z_i} \right] - \frac{\nu_g}{2}  E \left [U_i| \boldsymbol{X_i}, \boldsymbol {Z_i} \right] \right)
$$

for $g = 1, \dots G$.

The implementation provided does this by performing a line search on a specified range.

##Application

```{r, echo = F}
knitr::opts_chunk$set(echo = T,results = "hide")
library(RColorBrewer)
library(MASS)

source("R/mvtmixEM.R")
source("R/rmvtmix.R")
```

In order to demonstrate the tMM's robustness to outliers, we will use the Faithful data set in R with a manually added outlier (in red)

```{r}
x = rbind(faithful, c(2, 300))
plot(faithful, ylim = c(40,320), pch=19, cex=.4)
points(c(2,300), col = "red", pch=19, cex=1)
```





Randomally choosing the following starting point

```{r}
mu1 = c(5,3.2)
mu2 = c(15,12)
sigma1 = matrix( c(1,3/5,3/5,2), nrow =2)
sigma2 = matrix( c(1,3/5,3/5,2), nrow =2)
nu1 = 3
nu2 = 3

lambda0 = c(0.5,0.5)
mu0 = list(mu1, mu2)
sigma0 = list(sigma1, sigma2)
nu0 = c(nu1, nu2)
```

Yields a GMM, fitted using the mixtools package, with the following contour plot

```{r}
suppressMessages(library(mixtools))
theta.GMM = mvnormalmixEM(x = x, k = 2, lambda = lambda0, mu = mu0, sigma = sigma0)

rmvnormmix = function(n, lambda, mu, sigma) {
  G = length(lambda)
  t(sapply(1:n, function(...) {
  Zi = sample(x = 1:G, size = 1, prob = lambda)
  rmvnorm(n = 1, mu = mu[[Zi]], sigma = sigma[[Zi]])
  }))
}

fit.GMM = rmvnormmix(n = 10000, lambda = theta.GMM$lambda, 
                     mu = theta.GMM$mu, sigma = theta.GMM$sigma)

z = kde2d(fit.GMM [,1], fit.GMM [,2], n=50)

my.cols = rev(c("black", "black", "gray", "purple", "red", brewer.pal(11, "RdYlBu")))
plot(x, pch=19, cex=.4)

contour(z, drawlabels=FALSE, nlevels=11, col=my.cols, add=TRUE)
```

And a tMM (fitted using the mvtmixEM function here) with the following contour plot

```{r}
theta.tMM = mvtmixEM(x = x, lambda = lambda0, mu = mu0, sigma = sigma0, nu = nu0, 
                 nu_range = c(1:100), tolerance = 0.01, max_iterations = 100, maximize_nu = T)

fit.tMM =  rmvtmix(n = 10000, mu = theta.tMM$mu, sigma = theta.tMM$sigma, 
                   nu = theta.tMM$nu, lambda = theta.tMM$lambda)

z = kde2d(fit.tMM[,1], fit.tMM[,2], n=50)

my.cols = rev(c("black", "black", "gray", "purple", "red", brewer.pal(11, "RdYlBu")))
plot(x, pch=19, cex=.4)

contour(z, drawlabels=FALSE, nlevels=11, col=my.cols, add=TRUE)
```

Note how the variability of the component on the right is unreasonably large when using the GMM. The tMM does not have this problem.